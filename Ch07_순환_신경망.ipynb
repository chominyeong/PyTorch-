{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiBFfLFxMZGZkhBMvmKAme",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chominyeong/PyTorch-DeepLearning-Start/blob/main/Ch07_%EC%88%9C%ED%99%98_%EC%8B%A0%EA%B2%BD%EB%A7%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 07-01 RNN"
      ],
      "metadata": {
        "id": "mOo-0awXrZ5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 파이썬으로 RNN 구현하기"
      ],
      "metadata": {
        "id": "oJH3B0aErdrE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFUyfQbErQEP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "timesteps = 10 # 시점의 수. NLP에서는 보통 문장의 길이가 된다.\n",
        "input_size = 4 # 입력의 차원. NLP에서는 보통 단어 벡터의 차원이 된다.\n",
        "hidden_size = 8 # 은닉 상태의 크기. 메모리 셀의 용량이다.\n",
        "\n",
        "inputs = np.random.random((timesteps, input_size)) # 입력에 해당되는 2D 텐서\n",
        "\n",
        "hidden_state_t = np.zeros((hidden_size,)) # 초기 은닉 상태는 0(벡터)로 초기화\n",
        "# 은닉 상태의 크기 hidden_size로 은닉 상태를 만듬."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs) # 10개의 단어로 된 문장, 그리고 그 단어는 4차원 벡터이다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVy7xXbhrfjk",
        "outputId": "9d8b5288-b8d5-4ae0-9eff-b3398fd693fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.85482298 0.91474223 0.61272196 0.75971218]\n",
            " [0.12431433 0.02435886 0.7915536  0.29618307]\n",
            " [0.92637871 0.33426078 0.84202298 0.17935444]\n",
            " [0.08597966 0.48276765 0.95319959 0.95633523]\n",
            " [0.31718868 0.41383257 0.92795187 0.54814726]\n",
            " [0.86414138 0.42144139 0.30290708 0.81926815]\n",
            " [0.55305102 0.86599117 0.04820962 0.27046159]\n",
            " [0.52657503 0.68210718 0.63221473 0.79284059]\n",
            " [0.56088683 0.86285688 0.13136268 0.9086417 ]\n",
            " [0.11442805 0.75547308 0.71163161 0.64820262]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(hidden_state_t) # 8의 크기를 가지는 은닉 상태. 현재는 초기 은닉 상태로 모든 차원이 0의 값을 가짐."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJJIo2zErqUe",
        "outputId": "ac5cb02d-d6fb-43ae-a856-21c4ce827d57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치와 편향\n",
        "Wx = np.random.random((hidden_size, input_size))  # (8, 4)크기의 2D 텐서 생성. 입력에 대한 가중치.\n",
        "Wh = np.random.random((hidden_size, hidden_size)) # (8, 8)크기의 2D 텐서 생성. 은닉 상태에 대한 가중치.\n",
        "b = np.random.random((hidden_size,)) # (8,)크기의 1D 텐서 생성. 이 값은 편향(bias)."
      ],
      "metadata": {
        "id": "zdHv6a7Xr1HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.shape(Wx))  # (hidden state 크기 x 입력의 차원)\n",
        "print(np.shape(Wh))  # (hidden state 크기 x hidden state 크기)\n",
        "print(np.shape(b))   # (hidden state 크기,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGItPElIsw-5",
        "outputId": "d5f1ff6b-fac9-4059-dc39-9aca1e183121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 4)\n",
            "(8, 8)\n",
            "(8,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* RNN 수행"
      ],
      "metadata": {
        "id": "XsOQ5OUavnZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_hidden_states = []\n",
        "\n",
        "# 메모리 셀 동작\n",
        "for input_t in inputs: # 각 시점에 따라서 입력값이 입력됨.\n",
        "\n",
        "  # Wx * Xt + Wh * Ht-1 + b(bias)\n",
        "  output_t = np.tanh(np.dot(Wx,input_t) + np.dot(Wh,hidden_state_t) + b)\n",
        "\n",
        "  # 각 시점의 은닉 상태의 값을 계속해서 축적\n",
        "  total_hidden_states.append(list(output_t))\n",
        "\n",
        "  # 각 시점 t별 메모리 셀의 출력의 크기는 (timestep, output_dim)\n",
        "  print(np.shape(total_hidden_states))\n",
        "  print(total_hidden_states)\n",
        "\n",
        "  hidden_state_t = output_t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_jO0WrxszGl",
        "outputId": "fc95b625-68ec-4e3f-8f10-bd223f33d535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 8)\n",
            "[[0.9999993679321734, 0.9999959258957578, 0.9993652927499846, 0.9999944747771686, 0.9999986810134721, 0.999992632909479, 0.9995934203848083, 0.9999817116449556]]\n",
            "(2, 8)\n",
            "[[0.9999993679321734, 0.9999959258957578, 0.9993652927499846, 0.9999944747771686, 0.9999986810134721, 0.999992632909479, 0.9995934203848083, 0.9999817116449556], [0.9999904364105806, 0.999978417090773, 0.9931737377908968, 0.9999366295983699, 0.9999828345560092, 0.9999911800125427, 0.9992730653310608, 0.9998121078615558]]\n",
            "(3, 8)\n",
            "[[0.9999993679321734, 0.9999959258957578, 0.9993652927499846, 0.9999944747771686, 0.9999986810134721, 0.999992632909479, 0.9995934203848083, 0.9999817116449556], [0.9999904364105806, 0.999978417090773, 0.9931737377908968, 0.9999366295983699, 0.9999828345560092, 0.9999911800125427, 0.9992730653310608, 0.9998121078615558], [0.9999956002517499, 0.9999938264367458, 0.9986885468551093, 0.9999734357104993, 0.9999972253178662, 0.9999929930326468, 0.9993318306822336, 0.9999439198445242]]\n",
            "(4, 8)\n",
            "[[0.9999993679321734, 0.9999959258957578, 0.9993652927499846, 0.9999944747771686, 0.9999986810134721, 0.999992632909479, 0.9995934203848083, 0.9999817116449556], [0.9999904364105806, 0.999978417090773, 0.9931737377908968, 0.9999366295983699, 0.9999828345560092, 0.9999911800125427, 0.9992730653310608, 0.9998121078615558], [0.9999956002517499, 0.9999938264367458, 0.9986885468551093, 0.9999734357104993, 0.9999972253178662, 0.9999929930326468, 0.9993318306822336, 0.9999439198445242], [0.9999985434649342, 0.9999917376041622, 0.9969685809566474, 0.999991208744398, 0.9999938889982716, 0.999993816651424, 0.9997715974897378, 0.9999733689492007]]\n",
            "(5, 8)\n",
            "[[0.9999993679321734, 0.9999959258957578, 0.9993652927499846, 0.9999944747771686, 0.9999986810134721, 0.999992632909479, 0.9995934203848083, 0.9999817116449556], [0.9999904364105806, 0.999978417090773, 0.9931737377908968, 0.9999366295983699, 0.9999828345560092, 0.9999911800125427, 0.9992730653310608, 0.9998121078615558], [0.9999956002517499, 0.9999938264367458, 0.9986885468551093, 0.9999734357104993, 0.9999972253178662, 0.9999929930326468, 0.9993318306822336, 0.9999439198445242], [0.9999985434649342, 0.9999917376041622, 0.9969685809566474, 0.999991208744398, 0.9999938889982716, 0.999993816651424, 0.9997715974897378, 0.9999733689492007], [0.9999972158732326, 0.9999912214580692, 0.9973219005282132, 0.9999832900688282, 0.9999941174062478, 0.9999932909583685, 0.9996163198250176, 0.9999513983019231]]\n",
            "(6, 8)\n",
            "[[0.9999993679321734, 0.9999959258957578, 0.9993652927499846, 0.9999944747771686, 0.9999986810134721, 0.999992632909479, 0.9995934203848083, 0.9999817116449556], [0.9999904364105806, 0.999978417090773, 0.9931737377908968, 0.9999366295983699, 0.9999828345560092, 0.9999911800125427, 0.9992730653310608, 0.9998121078615558], [0.9999956002517499, 0.9999938264367458, 0.9986885468551093, 0.9999734357104993, 0.9999972253178662, 0.9999929930326468, 0.9993318306822336, 0.9999439198445242], [0.9999985434649342, 0.9999917376041622, 0.9969685809566474, 0.999991208744398, 0.9999938889982716, 0.999993816651424, 0.9997715974897378, 0.9999733689492007], [0.9999972158732326, 0.9999912214580692, 0.9973219005282132, 0.9999832900688282, 0.9999941174062478, 0.9999932909583685, 0.9996163198250176, 0.9999513983019231], [0.9999984274857319, 0.999991232235024, 0.9987423395563345, 0.9999827238477972, 0.9999974446302923, 0.9999897899901822, 0.9992758408091086, 0.9999553683562807]]\n",
            "(7, 8)\n",
            "[[0.9999993679321734, 0.9999959258957578, 0.9993652927499846, 0.9999944747771686, 0.9999986810134721, 0.999992632909479, 0.9995934203848083, 0.9999817116449556], [0.9999904364105806, 0.999978417090773, 0.9931737377908968, 0.9999366295983699, 0.9999828345560092, 0.9999911800125427, 0.9992730653310608, 0.9998121078615558], [0.9999956002517499, 0.9999938264367458, 0.9986885468551093, 0.9999734357104993, 0.9999972253178662, 0.9999929930326468, 0.9993318306822336, 0.9999439198445242], [0.9999985434649342, 0.9999917376041622, 0.9969685809566474, 0.999991208744398, 0.9999938889982716, 0.999993816651424, 0.9997715974897378, 0.9999733689492007], [0.9999972158732326, 0.9999912214580692, 0.9973219005282132, 0.9999832900688282, 0.9999941174062478, 0.9999932909583685, 0.9996163198250176, 0.9999513983019231], [0.9999984274857319, 0.999991232235024, 0.9987423395563345, 0.9999827238477972, 0.9999974446302923, 0.9999897899901822, 0.9992758408091086, 0.9999553683562807], [0.9999982176205137, 0.9999810150633198, 0.9983416318169619, 0.9999698429786714, 0.9999943932684877, 0.9999840119546196, 0.9982313505215212, 0.999779774454238]]\n",
            "(8, 8)\n",
            "[[0.9999993679321734, 0.9999959258957578, 0.9993652927499846, 0.9999944747771686, 0.9999986810134721, 0.999992632909479, 0.9995934203848083, 0.9999817116449556], [0.9999904364105806, 0.999978417090773, 0.9931737377908968, 0.9999366295983699, 0.9999828345560092, 0.9999911800125427, 0.9992730653310608, 0.9998121078615558], [0.9999956002517499, 0.9999938264367458, 0.9986885468551093, 0.9999734357104993, 0.9999972253178662, 0.9999929930326468, 0.9993318306822336, 0.9999439198445242], [0.9999985434649342, 0.9999917376041622, 0.9969685809566474, 0.999991208744398, 0.9999938889982716, 0.999993816651424, 0.9997715974897378, 0.9999733689492007], [0.9999972158732326, 0.9999912214580692, 0.9973219005282132, 0.9999832900688282, 0.9999941174062478, 0.9999932909583685, 0.9996163198250176, 0.9999513983019231], [0.9999984274857319, 0.999991232235024, 0.9987423395563345, 0.9999827238477972, 0.9999974446302923, 0.9999897899901822, 0.9992758408091086, 0.9999553683562807], [0.9999982176205137, 0.9999810150633198, 0.9983416318169619, 0.9999698429786714, 0.9999943932684877, 0.9999840119546196, 0.9982313505215212, 0.999779774454238], [0.9999989096178955, 0.9999929403333294, 0.9985916291894058, 0.9999907529804027, 0.9999969513969986, 0.9999920992444424, 0.9995831408401251, 0.9999685175778772]]\n",
            "(9, 8)\n",
            "[[0.9999993679321734, 0.9999959258957578, 0.9993652927499846, 0.9999944747771686, 0.9999986810134721, 0.999992632909479, 0.9995934203848083, 0.9999817116449556], [0.9999904364105806, 0.999978417090773, 0.9931737377908968, 0.9999366295983699, 0.9999828345560092, 0.9999911800125427, 0.9992730653310608, 0.9998121078615558], [0.9999956002517499, 0.9999938264367458, 0.9986885468551093, 0.9999734357104993, 0.9999972253178662, 0.9999929930326468, 0.9993318306822336, 0.9999439198445242], [0.9999985434649342, 0.9999917376041622, 0.9969685809566474, 0.999991208744398, 0.9999938889982716, 0.999993816651424, 0.9997715974897378, 0.9999733689492007], [0.9999972158732326, 0.9999912214580692, 0.9973219005282132, 0.9999832900688282, 0.9999941174062478, 0.9999932909583685, 0.9996163198250176, 0.9999513983019231], [0.9999984274857319, 0.999991232235024, 0.9987423395563345, 0.9999827238477972, 0.9999974446302923, 0.9999897899901822, 0.9992758408091086, 0.9999553683562807], [0.9999982176205137, 0.9999810150633198, 0.9983416318169619, 0.9999698429786714, 0.9999943932684877, 0.9999840119546196, 0.9982313505215212, 0.999779774454238], [0.9999989096178955, 0.9999929403333294, 0.9985916291894058, 0.9999907529804027, 0.9999969513969986, 0.9999920992444424, 0.9995831408401251, 0.9999685175778772], [0.9999993188880536, 0.9999889434532405, 0.9987570562232417, 0.9999896953850541, 0.9999968697014625, 0.9999875958303268, 0.9992514500192428, 0.9999468689881643]]\n",
            "(10, 8)\n",
            "[[0.9999993679321734, 0.9999959258957578, 0.9993652927499846, 0.9999944747771686, 0.9999986810134721, 0.999992632909479, 0.9995934203848083, 0.9999817116449556], [0.9999904364105806, 0.999978417090773, 0.9931737377908968, 0.9999366295983699, 0.9999828345560092, 0.9999911800125427, 0.9992730653310608, 0.9998121078615558], [0.9999956002517499, 0.9999938264367458, 0.9986885468551093, 0.9999734357104993, 0.9999972253178662, 0.9999929930326468, 0.9993318306822336, 0.9999439198445242], [0.9999985434649342, 0.9999917376041622, 0.9969685809566474, 0.999991208744398, 0.9999938889982716, 0.999993816651424, 0.9997715974897378, 0.9999733689492007], [0.9999972158732326, 0.9999912214580692, 0.9973219005282132, 0.9999832900688282, 0.9999941174062478, 0.9999932909583685, 0.9996163198250176, 0.9999513983019231], [0.9999984274857319, 0.999991232235024, 0.9987423395563345, 0.9999827238477972, 0.9999974446302923, 0.9999897899901822, 0.9992758408091086, 0.9999553683562807], [0.9999982176205137, 0.9999810150633198, 0.9983416318169619, 0.9999698429786714, 0.9999943932684877, 0.9999840119546196, 0.9982313505215212, 0.999779774454238], [0.9999989096178955, 0.9999929403333294, 0.9985916291894058, 0.9999907529804027, 0.9999969513969986, 0.9999920992444424, 0.9995831408401251, 0.9999685175778772], [0.9999993188880536, 0.9999889434532405, 0.9987570562232417, 0.9999896953850541, 0.9999968697014625, 0.9999875958303268, 0.9992514500192428, 0.9999468689881643], [0.9999986127093886, 0.9999888923954092, 0.997426070258065, 0.9999883734315285, 0.9999932917209734, 0.9999916565078952, 0.9995677348012122, 0.9999412500691257]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 각 시점의 출력값 확인\n",
        "total_hidden_states = np.stack(total_hidden_states, axis = 0)  # 출력 시 값을 깔끔하게 해준다.\n",
        "print(total_hidden_states) # (timesteps, output_dim)의 크기. 이 경우 (10, 8)의 크기를 가지는 메모리 셀의 2D 텐서를 출력."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyKTuhoBt2Y-",
        "outputId": "fee7e40d-3278-4c0f-e2b7-33abfd176558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.99999937 0.99999593 0.99936529 0.99999447 0.99999868 0.99999263\n",
            "  0.99959342 0.99998171]\n",
            " [0.99999044 0.99997842 0.99317374 0.99993663 0.99998283 0.99999118\n",
            "  0.99927307 0.99981211]\n",
            " [0.9999956  0.99999383 0.99868855 0.99997344 0.99999723 0.99999299\n",
            "  0.99933183 0.99994392]\n",
            " [0.99999854 0.99999174 0.99696858 0.99999121 0.99999389 0.99999382\n",
            "  0.9997716  0.99997337]\n",
            " [0.99999722 0.99999122 0.9973219  0.99998329 0.99999412 0.99999329\n",
            "  0.99961632 0.9999514 ]\n",
            " [0.99999843 0.99999123 0.99874234 0.99998272 0.99999744 0.99998979\n",
            "  0.99927584 0.99995537]\n",
            " [0.99999822 0.99998102 0.99834163 0.99996984 0.99999439 0.99998401\n",
            "  0.99823135 0.99977977]\n",
            " [0.99999891 0.99999294 0.99859163 0.99999075 0.99999695 0.9999921\n",
            "  0.99958314 0.99996852]\n",
            " [0.99999932 0.99998894 0.99875706 0.9999897  0.99999687 0.9999876\n",
            "  0.99925145 0.99994687]\n",
            " [0.99999861 0.99998889 0.99742607 0.99998837 0.99999329 0.99999166\n",
            "  0.99956773 0.99994125]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 파이토치의 nn.RNN()"
      ],
      "metadata": {
        "id": "ig434C6nwAny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "h_FH8QXgwCNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 5 # 입력의 크기\n",
        "hidden_size = 8 # 은닉 상태의 크기"
      ],
      "metadata": {
        "id": "B8652Ct8wCLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (batch_size, time_steps, input_size)\n",
        "inputs = torch.Tensor(1, 10, input_size)"
      ],
      "metadata": {
        "id": "JD9tOJf3wCIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN 실행"
      ],
      "metadata": {
        "id": "VQ54b5MFxZO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN 셀 생성\n",
        "cell = nn.RNN(input_size, hidden_size, batch_first=True)  # batch_first = True : 입력 텐서의 첫 번째 차원이 배치 크기이다."
      ],
      "metadata": {
        "id": "UfZi-XoUwCGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input 넣기\n",
        "outputs, _status = cell(inputs)"
      ],
      "metadata": {
        "id": "7z2qz30UwCD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.shape) # 모든 time-step의 hidden_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfK-drBtwCBy",
        "outputId": "6c7cb9e4-15af-495f-94f8-d6817034d0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10번의 시점동안 8차원의 은닉상태가 출력됨"
      ],
      "metadata": {
        "id": "kTP7zbvwx_KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(_status.shape) # 최종 time-step의 hidden_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQwrKZ6-t7QC",
        "outputId": "9c560b69-7654-4b50-deee-6ed02fce0ccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 깊은 순환 신경망"
      ],
      "metadata": {
        "id": "e10t7vzrzhxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (batch_size, time_steps, input_size)\n",
        "inputs = torch.Tensor(1, 10, 5)\n",
        "\n",
        "## num_layers = 2\n",
        "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True)\n",
        "outputs, _status = cell(inputs)\n",
        "\n",
        "print(outputs.shape) # 모든 time-step의 hidden_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiAa6FIlyC0j",
        "outputId": "324bd514-3934-4a1e-cc93-13133592cd85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(_status.shape) # (층의 개수, 배치 크기, 은닉 상태의 크기)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYpRalF1zt50",
        "outputId": "69645a1c-d6af-4e37-d9fe-5cdd1bcc54c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 깊은 양방향 순환 신경망"
      ],
      "metadata": {
        "id": "n3ZEQ0141pW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (batch_size, time_steps, input_size)\n",
        "inputs = torch.Tensor(1, 10, 5)\n",
        "\n",
        "## bidirectional = True\n",
        "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True, bidirectional = True)\n",
        "outputs, _status = cell(inputs)"
      ],
      "metadata": {
        "id": "gnxXCUA0z-E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.shape) # (배치 크기, 시퀀스 길이, 은닉 상태의 크기 x 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaAnBBYK1y7b",
        "outputId": "9620016e-27e9-4db8-e102-8e3d2b5349e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(_status.shape) # (층의 개수 x 2, 배치 크기, 은닉 상태의 크기)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzdOKwld10AM",
        "outputId": "0745c504-7c17-44f6-d098-8dbdbae8a362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM"
      ],
      "metadata": {
        "id": "sJ2hmzmJ2OBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.LSTM(input_dim, hidden_size, batch_fisrt=True)"
      ],
      "metadata": {
        "id": "rPZ6gfGm1-_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU"
      ],
      "metadata": {
        "id": "crDUTa6_3agf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.GRU(input_dim, hidden_size, batch_fisrt=True)"
      ],
      "metadata": {
        "id": "mu81yu7e3bK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Char RNN(1)"
      ],
      "metadata": {
        "id": "y_GurFpV3dDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Lrf86k1B3emj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 훈련 데이터 전처리"
      ],
      "metadata": {
        "id": "gvyA2bNQ7oj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = 'apple'\n",
        "label_str = 'pple!'\n",
        "char_vocab = sorted(list(set(input_str+label_str)))\n",
        "vocab_size = len(char_vocab)\n",
        "print ('문자 집합의 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRZc1qDs7nzN",
        "outputId": "f9b92616-c55f-4d7a-e0ba-e9ba8dc3e082"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기 : 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5-AqS4m7nwq",
        "outputId": "aa059c05-3a8a-4fcd-b888-5a3e4b6fb123"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['!', 'a', 'e', 'l', 'p']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = vocab_size # 입력의 크기는 문자 집합의 크기\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "BkamojdV7nuc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
        "print(char_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKGA3x51886f",
        "outputId": "c8d51ee2-f762-4f2d-acd6-75d85812620f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 나중에 예측 결과(정수)를 다시 문자 시퀀스로 보기 위해\n",
        "index_to_char={}\n",
        "for key, value in char_to_index.items():\n",
        "    index_to_char[value] = key\n",
        "print(index_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NedBl54r883m",
        "outputId": "36d6d081-42fc-41fa-f299-55aa37c66c6c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "In9zvn6i9NEl",
        "outputId": "c16f9143-f716-4691-db8a-3f5ee4c89880"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'apple'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in label_str]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9_RE65y880-",
        "outputId": "cc2e7768-626d-4bd7-df36-524440f86ae8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치의 nn.RNN()은 기본적으로 3차원 텐서를 입력받는다."
      ],
      "metadata": {
        "id": "9YiaBLzD9Trt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치 차원 추가\n",
        "# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있었음.\n",
        "x_data = [x_data]\n",
        "y_data = [y_data]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAj5lBI588ya",
        "outputId": "698bf6bb-ee84-42cb-be39-40555da435d1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 4, 4, 3, 2]]\n",
            "[[4, 4, 3, 2, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 시퀀스의 각 문자들을 원-핫 벡터로 변경\n",
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "print(x_one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GshKJmKY88tH",
        "outputId": "bdfd0dd6-e648-4044-d354-50d268372b0c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0., 0.]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 데이터와 레이블 데이터 → 텐서로 변환\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2730bKz7nr6",
        "outputId": "29df988f-532b-4dcb-9320-24ac5629dcbb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-3e6fba19e999>:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  X = torch.FloatTensor(x_one_hot)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZz5Box09wIG",
        "outputId": "5af84fcb-7471-4d0c-ec50-2d823701c080"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
            "레이블의 크기 : torch.Size([1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 모델 구현하기"
      ],
      "metadata": {
        "id": "te1ljHlh9zG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\n",
        "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
        "            #fc : fully-connected layer\n",
        "\n",
        "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "QqGweF8C9wFU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(input_size, hidden_size, output_size)"
      ],
      "metadata": {
        "id": "KF0KSdhX9wCm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서\n",
        "# (배치 차원, 시점(timesteps), 출력의 크기)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy8KExul9v_z",
        "outputId": "c3249630-bd72-489b-ce72-5b87adfaeaac"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpNlqKOT9v7I",
        "outputId": "d3b9d3df-13bb-4ddf-8321-aabd92cff1b2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1MBj1df9v4M",
        "outputId": "cc252855-c6a7-403b-f30a-c43dba615991"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5])\n",
            "torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "l2_WX4Y6-NiY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 100번의 에포크 학습\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
        "    loss.backward() # 기울기 계산\n",
        "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
        "\n",
        "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
        "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
        "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSEaboHU-KKp",
        "outputId": "94499ab9-497f-477a-c724-7baec87bdbcc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss:  1.6723861694335938 prediction:  [[2 4 4 3 3]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  eppll\n",
            "1 loss:  1.3790686130523682 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
            "2 loss:  1.2026500701904297 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
            "3 loss:  1.0397974252700806 prediction:  [[4 4 3 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplpp\n",
            "4 loss:  0.8639044761657715 prediction:  [[4 4 3 2 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplep\n",
            "5 loss:  0.7000270485877991 prediction:  [[4 4 3 2 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplep\n",
            "6 loss:  0.5418907403945923 prediction:  [[4 4 3 2 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplep\n",
            "7 loss:  0.4186881482601166 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "8 loss:  0.32870712876319885 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "9 loss:  0.25254496932029724 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "10 loss:  0.19121305644512177 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "11 loss:  0.14424912631511688 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "12 loss:  0.1081308126449585 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "13 loss:  0.08082371205091476 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "14 loss:  0.06039878726005554 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "15 loss:  0.04532383009791374 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "16 loss:  0.034490279853343964 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "17 loss:  0.02684180997312069 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "18 loss:  0.021417247131466866 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "19 loss:  0.01748398318886757 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "20 loss:  0.014547189697623253 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "21 loss:  0.012291504070162773 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "22 loss:  0.010518018156290054 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "23 loss:  0.009098587557673454 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "24 loss:  0.007947547361254692 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "25 loss:  0.007004766725003719 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "26 loss:  0.006226128898561001 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "27 loss:  0.005578590556979179 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "28 loss:  0.005036560818552971 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "29 loss:  0.0045798481442034245 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "30 loss:  0.004192577674984932 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "31 loss:  0.003862121608108282 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "32 loss:  0.0035782940685749054 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "33 loss:  0.0033329962752759457 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "34 loss:  0.003119590226560831 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "35 loss:  0.002932980190962553 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "36 loss:  0.0027686706744134426 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "37 loss:  0.002623223466798663 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "38 loss:  0.002493856009095907 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "39 loss:  0.0023780653718858957 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "40 loss:  0.002274175640195608 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "41 loss:  0.002180342096835375 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "42 loss:  0.002095357980579138 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "43 loss:  0.0020179920829832554 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "44 loss:  0.0019474163418635726 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "45 loss:  0.0018827058374881744 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "46 loss:  0.0018232918810099363 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "47 loss:  0.0017685580532997847 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "48 loss:  0.001717982580885291 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "49 loss:  0.0016710907220840454 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "50 loss:  0.0016275501111522317 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "51 loss:  0.0015870521310716867 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "52 loss:  0.0015492407837882638 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "53 loss:  0.0015138781163841486 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "54 loss:  0.0014807748375460505 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "55 loss:  0.0014498118543997407 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "56 loss:  0.0014206323539838195 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "57 loss:  0.0013931416906416416 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "58 loss:  0.0013672448694705963 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "59 loss:  0.0013427513185888529 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "60 loss:  0.001319590024650097 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "61 loss:  0.0012977371225133538 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "62 loss:  0.0012769073946401477 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "63 loss:  0.0012571720872074366 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "64 loss:  0.0012384122237563133 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "65 loss:  0.0012204615632072091 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "66 loss:  0.0012034388491883874 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "67 loss:  0.001187153859063983 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "68 loss:  0.001171606476418674 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "69 loss:  0.001156725687906146 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "70 loss:  0.0011424397816881537 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "71 loss:  0.0011287250090390444 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "72 loss:  0.0011156052350997925 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "73 loss:  0.001102890120819211 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "74 loss:  0.0010907461401075125 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "75 loss:  0.0010790069354698062 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "76 loss:  0.001067696139216423 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "77 loss:  0.001056742388755083 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "78 loss:  0.0010462409118190408 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "79 loss:  0.00103595363907516 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "80 loss:  0.0010259996633976698 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "81 loss:  0.0010164504637941718 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "82 loss:  0.0010071153519675136 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "83 loss:  0.0009980660397559404 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "84 loss:  0.0009892546804621816 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "85 loss:  0.0009806575253605843 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "86 loss:  0.0009722748072817922 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "87 loss:  0.0009641299839131534 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "88 loss:  0.0009561995975673199 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "89 loss:  0.0009484119946137071 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "90 loss:  0.0009408147889189422 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "91 loss:  0.0009333842317573726 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "92 loss:  0.0009260727092623711 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "93 loss:  0.000918975449167192 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "94 loss:  0.0009119497844949365 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "95 loss:  0.0009050905937328935 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "96 loss:  0.0008983742445707321 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "97 loss:  0.0008917294326238334 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "98 loss:  0.00088525126921013 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "99 loss:  0.00087886827532202 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Char RNN(2)"
      ],
      "metadata": {
        "id": "kApH0eJ43fBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "BuU3vJfC3gu-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 훈련 데이터 전처리"
      ],
      "metadata": {
        "id": "mIrwhbgG-YLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")"
      ],
      "metadata": {
        "id": "ok6dmMAQ-WYo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\n",
        "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩\n",
        "\n",
        "print(char_dic) # 공백도 여기서는 하나의 원소"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ntluv2MX-aJD",
        "outputId": "33399f8d-ca43-427d-86d6-0f1e6f9a8e81"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'d': 0, 'w': 1, 'y': 2, 'a': 3, 'p': 4, 'k': 5, 'f': 6, 'n': 7, ' ': 8, 'm': 9, ',': 10, 'o': 11, '.': 12, 'g': 13, 'l': 14, \"'\": 15, 't': 16, 'e': 17, 'h': 18, 'c': 19, 'u': 20, 's': 21, 'i': 22, 'b': 23, 'r': 24}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic_size = len(char_dic)\n",
        "print('문자 집합의 크기 : {}'.format(dic_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k77Vaicf-csA",
        "outputId": "54dbd2d8-b299-437a-91a3-cd04353dce9c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기 : 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "hidden_size = dic_size\n",
        "sequence_length = 10  # 임의 숫자 지정 : 샘플을 10개 단위로 끊기 위해\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "ACeELVkM-gyj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 구성\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):  # 10의 단위로 샘플을 자른다.\n",
        "    x_str = sentence[i:i + sequence_length]\n",
        "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
        "    print(i, x_str, '->', y_str)\n",
        "\n",
        "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
        "    y_data.append([char_dic[c] for c in y_str])  # y str to index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEj1ZuUz-pya",
        "outputId": "d23399f9-4f65-4ed6-81b4-74a91222fc5f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_data[0])   # if you wan에 해당됨.\n",
        "print(y_data[0])   # f you want에 해당됨."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQvVKwDi-pwL",
        "outputId": "e59fd5d0-8886-4797-b722-fa5deab7ab77"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[22, 6, 8, 2, 11, 20, 8, 1, 3, 7]\n",
            "[6, 8, 2, 11, 20, 8, 1, 3, 7, 16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "YXkkVjCG-pcN"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYyOx5N---dh",
        "outputId": "cd77ed45-cd40-48be-814e-418eb208f7d5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
            "레이블의 크기 : torch.Size([170, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSJidP7F-_oC",
        "outputId": "ab4bbc5c-0795-4589-c2a1-461af33a2a51"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeXjCxWD_BG1",
        "outputId": "6c150b10-e37a-4349-bc2d-beed2c081638"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 6,  8,  2, 11, 20,  8,  1,  3,  7, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 모델 구현하기"
      ],
      "metadata": {
        "id": "EzWu6II4_C7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic_size와 같음.\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "hQWgsoti_ChE"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(dic_size, hidden_size, 2) # 이번에는 층을 두 개 쌓습니다.(은닉층 2개)"
      ],
      "metadata": {
        "id": "YMyZ1w1u_LWg"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "-DzTMPEB_IgZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDc6Sm1z_Ndb",
        "outputId": "319d5f10-6d5d-4e58-f8ae-96111017d89c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 10, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-zhChBs_QJ7",
        "outputId": "962a8b43-1d67-4c94-fc65-10544eeaa4b2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1700, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv700mVK_Sd2",
        "outputId": "295265e7-90bc-48f0-b0da-242063f45f1b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 10])\n",
            "torch.Size([1700])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # results의 텐서 크기는 (170, 10)\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
        "            predict_str += ''.join([char_set[t] for t in result])\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "            predict_str += char_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC5lIQ0w_UmS",
        "outputId": "4f70292f-03e0-40c6-a39b-2d9900012c6b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gg'ggg'gggggggggggggggggggggggggggggggggg'g'gg'ggggggggggggggggggggggggggggggggggggggggggggggggg'ggggggggggggggg'gggggggggg'ggggg'gggg'gggggggggggggggggggggggggggggggggggg'ggggggg\n",
            "                                                                                                                                                                                   \n",
            "     e ele ele ele  e e ele   e e e  e   el        el e   e  l e  le  e     e ele   e  e       e e e e   ele    eele           eel     e l e  l   e  e e    e          e      e    \n",
            "dt..bdt..h       h  ho   oh d ho  d hh   h           h   h  d h   hh   h   hh     d d      h  h  h  h  h   hh   hhh   hh  h   hh  h o  h  h    h  h       do    h       ho    fh   \n",
            "ttrlatdmattttttttdtdttttdtttdtdttttmdttttttttdtdttttttttmttdlttdttdtdltttttttdttttttdtttttttdtttttlttdttdtt ttttdtttltdtdmdtdtdtdttdtdttlmtdttdtdttttlttdtdtdtdtttdttttttdttmtldttt\n",
            "tt t t o tttttttt tttttt thttt tttttttttttttt tttttttttttttttttttt ttttttttttttttttttttttttttttttttttttttttttttttttttt tttttttttttt tttttttttttttttttttttt tttttttttttttttttttttttt\n",
            "t  tt tot      att    t t t ttl t t t  t     t     t   t    t     t  t             tt     t           t    t    t   t t t     ht   l   t              t   t        t         t     \n",
            "    i   t           t t l t   t                                                   t                             t         i                           t                            \n",
            "                                                                                                                      t   s                       t    r                         s \n",
            "          e  e            , e t e            s       e e          ns   n        e                       n e n     n e s e s                   e        re   e e           e  e     \n",
            "  e    o    oeoo  eo o rto, e t eotoe  t    ot    oooe e esoteo o   ot e  oo   oeo  toe eot e oet ees t e e t e toe ers e soo  t tot et   s o e ers t ereot e e e e e e   eeoe e s \n",
            "  eo tdo  otdeootoeo st eo, w t eoeoe  e tte  ee eoeoe e etoetde e  ot eoeodettoe s eo  n t d d t s s soe t toe eoe eotoe tto   t oe st  t e ee t t eo oeot e e ete e e  oeeoeo  s \n",
            "   e  do  etoeeoe eotde e , wot eoteet e tte  ee eoeee er thetdt eredtoe eent eoe t ts  d   daoet s sseheot t e eh  eot e nthe  eeoers  e     eoe woe eoeo  e e t ts teotdeeoeoe e \n",
            "    o  o    r doe eotse epthwot   toot th t e eeet t e t stdt o  ee htot eant wot   ese s   o  s ds   tht wothe eot eot e t he teeoe s   t e ha t eoe e t w   w , esh eato doehw e \n",
            "      do    oewoe woaot eos eotp  toot t ta e eo th     ooloeeo  r t tot ean, eot   tse s      st     ts  aot t eo, aot e  eoe   eot  t e     aht eotst , e  he ,       to  oe eo  \n",
            "      to   to  oo w tot ep, wo t  toot ao   n    ahe s t  tooao  r t to   am  eo  t ts   ot  eps      ths wo    wp  eot ep  o    aod  t el t  tot wod t   e   to,  o    ao  oept   \n",
            "nsto  ton  to  o  e aot ep, to    to   to te   e t e s t  toeeo  eot to   wn, ao  t tn  e  toem tot   ap  wo    to  tot tpnto    wot  t ewo    o  t t tht tes tuh      eao toept   \n",
            "n,to  do   to eo  e a d ep, bo    boo  to tos tent d r testoeeo le t to  oa   ao  t dns o  aoep tos   an  ao    to  aot e nto  t tot  t dlo   tod d t   d de  rnd   s   wo toest t \n",
            "nnto  oo t to tu  o a t ep, ro e  bout tn t   te t   , t  toseo eent dor  t   do  t dn  n  woep tos t tn  oo    do  aot e  to  t the sth eo   tnd d d tht en  on  s  t  oo toe t   \n",
            "snto  dont do tu  e a d ept wor t dout eo t   te t   s te toseo eo t eone ans ao tt dns  nstoep tos   tns aor   to  eot e  to  t ahe  th to   d d d e t d tr  en  s  th oo toe t   \n",
            "' toa dont do to  d ahd ep  dor't tout do te   e t   s ee toteo le t don  ans aor't dn  g stoep tos   tm, won   dot dut e  tor t eoe  totpo r dod doe t t er  en  ,   y wo toe t   \n",
            "' tou aont do audld ahd ep, won't tout wo te   e t      r to lo le t done aht aor't dns gnstoe  to    am, won   wot aut ep toe t toe  totpo r doa doe t t e s wn  ,  gylao aoest   \n",
            "thto  tont do tutld ahd ip, won't dout wr te t i t ,   er te to le t wond ahd aor't tss gn toe  t s t am, won , wut wut er te  t t s  to lo d tua doe t d eps wn  ,s g, wo toe t s \n",
            "thto  tont to tutld aht ip, don't tout tp te t i th,e  er to to le t wood and won't te,lgn toe  t s t as, won , dut wat er ton t ahs  to le d tud doe t d eps wnn ,s gy wo toe t t \n",
            "thto  tont to tu ld ant ip, don't tout te te s e to es er to lo ee t woot and don't tei gn toer tos t an, don   dut dan er toan  ahem to lo d ton doe tnd ess tnn gs ty to toe t t \n",
            "thao  tont to tu lo tmt'ipt don't tout t  te   e th esher to lo ee t doot and don't tni 'n them th  t tnt doo , tut dat er toan ethensth lo d ton dhemtnt e s tmm gt gy wo the t s \n",
            "thao  tont to tutld antnipt don't tout apstea  e th ester to co ee t wood and won't tns gn them tos   and woo , tut wat er toa  ethemsto co d ton toe tnt e s tm  gs gy ao toe t g \n",
            "'hco  tont to tui d ant ip, don't aout apstea  e th ester to co  e t wood and won't ans gn them to    tnd woo , wut wat er toa  ethem to cond ton toe tns e s imi ns gy wo the t a \n",
            "'hco  dont th tui d ans epd don't aout ap tea  e th et er to lo le t wood and won't ans gn them to    and dook, wut dather toa   the  to lond ton aoe sns e s imi nsigy ao the s a \n",
            "thcor tont th tui d ans ip, don't aout ap tea  e th esher to lo le t wood and won't ans gn them to    and woog, but dather toa h toes to cong ton toe sns ess tm, nsisy ao the s a \n",
            "thcor wont ao tuild and ip, don't arut ap tea  e togesher to lo lert wood and aon't ansign them t s s and wook, but dadher toa h them to long torktoe sns e s immlnsigy wo toe s a \n",
            "thcur tont to tuild a s ip, don't arut ap tea  e tognsher te lo lert word and don't assign them ths s tnd dook, but dadher tea k them to long tor toe sns essiimm nsigy wo toe s m \n",
            "thcor tont to tuild a s ip, don't arut ap tea  e thgedher to lo lect wood and won't assign them tos , tnd wook, but dather toa hethem to lond tor toe sns e s tmmensity wo toe s m \n",
            "thcor tont th tuild t s ip, don't arut ap teacle togesher to lo lect word and don't assign them tos , tnd dork, but dather toanh them to lond tor toe snd e s immensity ao toe s m \n",
            "thlor wont to tuild a s ip, don't arut ap peacle together to lo lect word and won't amsign them tos , and work, but dather toanh them to lond tor toe snd ess immensity wo toe s mi\n",
            "thlor tont th tuild a ship, don't drut tp peacle together to lo lect wood and don't dmsigncthem tos   and work, but dather teach them to long tor toe sndless immensity af the s m \n",
            "thlou tant th build a ship, don't drut tp peac e together te lo lect wood and don't dssigncthem tos   and dork, but dather toach them to long tor the sndless immensity af the s m \n",
            "thcoa tant to build a ship, don't arut tp teac e together te co le t wood and don't assign them tes s and wor , but dather teach them to long tor toe sndless immensity af the s m \n",
            "thcoa tant to build a ship, don't arut tp peaceestogether te co le t wood and won't assign them tes s and work, but rather teach them to long tor toe sndless immensity af the s s \n",
            "t lod dant to tuild a ship, don't arum tp peaplestogether te lo lest wood and don't assign them tesks and dork, but rather teach them to long tor the sndless immensity af the s m \n",
            "t coa dant to build a ship, don't drum up peaplestogether to lollest wood and won't assign them tosks and work, but rather toach them to long tor toe lndless immensity af the s a \n",
            "t loa wont to tuild a ship, don't arum up peaplestogether to lollest wood and won't assign them tosks and work, but rather toach them to long tor the  ndless immensity af the s m \n",
            "t loa wont to build a ship, don't arum up peoplestogether to lollest wood and don't assign them tosks and work, but rather toach them to long tor the sndless immensity af the s o \n",
            "t loa want to tuild a ship, don't arum up people together to colle t wood and don't assign them tosks and work, but rather toach them to long tor the sndless immensity af thems o \n",
            "t loa want to build a ship, don't drum up people together to lollect wood and don't dssign them tosks and dork, but rather toach them to long tor the sndless immensity af thems a \n",
            "t loa aont to build a ship, don't drum up people together to collect wood and won't dssign them tosks and work, but rather toach them to long tor the endless immensity af the eea \n",
            "t loa want to build a ship, don't drum up people together to lollect wood and don't dssign them tosks and work, but rather toach them to long for the endless immensity of themeem \n",
            "tolor wont to buildea ship, don't drum up people together to collect wood and don't dss gn them tosks and dork, but rather toach them to long for the endless tmmensity af themseo \n",
            "t loa want to tuild a ship, don't drum up peaplestogether to lollest wood and don't dssign them tosks and work, but rather to th them to long tor the endless immensity of the semi\n",
            "p log wont to build a ship, don't drum up peoplestogether te lolle t wood and don't dssign them tasks and work, but rather teach them to lond for the sndless immensity of the seo \n",
            "polou want to build a ship, don't drum up people together to lollect wood and won't dssign them tosks and work, but rather toach them to cond for the endless immensity of the eeo \n",
            "pocor want to build a ship, don't drum up people together to collect wood and won't dssign them tosks and work, but rather toach them to cong for the endless immensity of the eeo \n",
            "t lor want to build a ship, don't drum up people together te lollect wood and won't dssign them tesks and dork, but rather teach them to cong tor the sndless immensity of the sea \n",
            "p lor want to build a ship, don't drum up people together te lollect wood and won't dssign them tosks and work, but rather teach them to long for the endless immensity of the sea \n",
            "d loa want to build a ship, don't drum up people together to lollect wood and won't assign them tosks and work, but rather toach them to long fou the endless immensity of the eea \n",
            "m lor want to build a ship, don't drum up people together to lollect wood and won't assign them tosks and work, but rather toach them to cong for the endless immensity of the ena \n",
            "m lol dont to build a ship, don't drum up people together to lollect wood and don't assign them tosks and dork, but rather toach them to cong for the endless immensity of the sea \n",
            "t lou want to build a ship, don't drum up people together to lollect wood and don't assign them tosks and dork, but rather toach them to long for the endless immensity of the sea \n",
            "t lou want to build a ship, don't drum up people together te lollect wood and don't assign them tosks and dork, but rather teach them to long for the endless immensity of the seo \n",
            "t lou want to build a ship, don't drum up people together te lollect wood and don't assign them tosks and dork, but rather teach them to long for the endless immensity of the eea \n",
            "t lou want to build a ship, don't drum up people together te lollect wood and don't assign them tosks and dork, but rather teach them to long for the endless immensity of the sea \n",
            "f lou want to build a ship, don't drum up people together te lollect wood and don't assign them tasks and dork, but rather teach them ta long for the endless immensity of the sea \n",
            "t lou want to build a ship, don't drum up people together te lollect wood and don't assign them tasks and dork, but rather teach them to long for the endless immensity of the sea \n",
            "t lou want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and dork, but rather teach them to long for the endless immensity of the sea \n",
            "t lou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and dork, but rather teach them to cong for the endless immensity of the sea \n",
            "t lou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to cong for the endless immensity of the sea \n",
            "p lou want to build a ship, don't drum up people together te collect wood and don't assign them tasks and dork, but rather teach them to cong for the endless immensity of the sea \n",
            "p lou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and dork, but rather teach them to cong for the endless immensity of the sea \n",
            "p lou want to build a ship, don't drum up people together te collect wood and don't assign them tasks and dork, but rather teach them to cong for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and dork, but rather teach them to cong for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to cong for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and dork, but rather teach them to cong for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to cong for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to cong for the endless immensity of the sea \n",
            "f you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to cong for the endless immensity of the sea \n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to cong for the endless immensity of the sea \n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to cong for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to cong for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to cong for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "g you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to cong for the endless immensity of the sea.\n",
            "f you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the sndless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "27n5fun-_d_d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}